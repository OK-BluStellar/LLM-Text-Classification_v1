# 医療文章分類システム v1

Docker医療文章分類システムは、WindowsおよびMac上でDockerコンテナとして動作する、ブラウザ完結型の医療アンケート文章分類システムです。ローカルLLM（Ollama）を使用して、医療問診やコールセンターの記録などを効率的に分類します。

## 主な特徴

- **Docker完全対応**: Windows/Mac両対応のmulti-archイメージ
- **ローカルLLM**: Ollama統合により、データを外部に送信せず安全に処理
- **ブラウザ完結**: インストール不要、コンテナ起動後すぐにアクセス可能
- **モバイル対応**: iOS/Android対応のレスポンシブデザイン
- **設定管理**: Hydraによる柔軟な設定管理
- **多様な入力形式**: CSV/Excelファイル対応

## システム構成

```
LLM-Text-Classification_v1/
├── app/
│   ├── core/                    # コアロジック
│   │   ├── ollama_client.py     # Ollama API クライアント
│   │   ├── data_processor.py    # データ処理
│   │   └── classifier.py        # 分類ロジック
│   ├── ui/                      # ユーザーインターフェース
│   │   └── gradio_app.py        # Gradio WebUI
│   └── main.py                  # エントリーポイント
├── config/                      # Hydra設定
│   └── config.yaml              # メイン設定ファイル
├── data/                        # データディレクトリ
├── Dockerfile                   # Dockerイメージ定義
├── docker-compose.yml           # Docker Compose設定
└── requirements.txt             # Python依存関係
```

## 技術スタック

- **実行環境**: Docker (Windows/Mac対応)
- **LLM実行**: Ollama
- **設定管理**: Hydra (config as code)
- **WebUI**: Gradio (レスポンシブデザイン)
- **データ処理**: pandas, openpyxl

## セットアップ手順

### 必要要件

- Docker Desktop (Windows/Mac)
- 最低8GB RAM推奨
- ディスク空き容量: 10GB以上

### インストール

1. **リポジトリのクローン**
```bash
git clone https://github.com/OK-BluStellar/LLM-Text-Classification_v1.git
cd LLM-Text-Classification_v1
```

2. **Docker Composeで起動**
```bash
docker compose up --build
```

初回起動時は以下の処理が実行されます:
- Dockerイメージのビルド
- Ollamaのインストール
- Python依存関係のインストール

3. **ブラウザでアクセス**
```
http://localhost:7860
```

起動後、すぐにWebUIが表示されます。Basic認証は不要です。

## 使い方

### 基本的な使用フロー

#### タブ1: 主要エリア

**1. データ入力**
- CSVまたはExcelファイルをアップロード
- デフォルトでテストデータが読み込まれています
- ファイルアップロード時、デフォルトデータは上書きされます

**2. プロンプト入力**
- 分類条件を日本語で記述
- 例: 「手関節に痛みを感じているか?」

**3. 分類実行**
- 「分類実行」ボタンをクリック
- LLMが各テキストを順次処理します

**4. 処理状況**
- リアルタイムで進捗状況を表示
- 現在処理中のテキストと結果を確認

**5. 出力**
- 「CSVダウンロード」ボタンで結果をダウンロード
- UTF-8 BOM付きCSVで出力（Excelで文字化けなし）

**6. 結果表示**
- 分類結果を表形式で表示
- ID、テキスト、分類結果、LLM応答を含む

#### タブ2: 設定パネル

**モデル設定**
- モデル名: 使用するOllamaモデルを指定（例: llama3:8b）
- Temperature: 生成のランダム性を調整（0.0-1.0）
- Max Tokens: 生成する最大トークン数
- Top P: 上位確率の累積しきい値

**システム状態**
- Ollama接続状態の確認
- 「接続確認」ボタンで接続テスト

### データフォーマット

#### 入力ファイル

CSV/Excelファイルに以下の列を含めてください:

| 列名 | 説明 | 必須 |
|------|------|------|
| ID | 識別子 | 推奨（なければ自動生成） |
| text / Text / テキスト / 文章 | 分類対象のテキスト | 必須 |

**例:**
```csv
ID,text
1,手首痛い。朝のこわばりも感じている。
2,ぶつけた後に手関節いたい。
3,今日はいい天気。朝、交通事故にあった。
```

#### 出力ファイル

結果は以下の列を含むCSVファイルとして出力されます:

| 列名 | 説明 |
|------|------|
| ID | 元のID |
| テキスト | 分類対象のテキスト |
| 分類結果 | 1（該当）または0（非該当） |
| LLM応答 | LLMの生応答 |

## テスト仕様

### テスト1: 基本動作確認

```bash
# コンテナ起動
docker compose up

# ブラウザでアクセス
# http://localhost:7860
```

**確認項目:**
- ✅ Webページが正常に表示される
- ✅ デフォルトテストデータが読み込まれている
- ✅ タブが切り替えられる
- ✅ 設定パネルで「接続確認」が成功する

### テスト2: テストデータの解析

デフォルトで以下のテストデータが読み込まれています:

| ID | テキスト |
|----|----------|
| 1 | 手首痛い。朝のこわばりも感じている。何もしてないのに症状出た。 |
| 2 | ぶつけた後に手関節いたい。どんな病気なのだろう? |
| 3 | 今日はいい天気。朝、交通事故にあった。首が何となく痛い。 |

**デフォルトプロンプト:**
```
手関節に痛みを感じているか?
```

**実行手順:**
1. 「主要エリア」タブを開く
2. プロンプト入力欄に「手関節に痛みを感じているか?」と入力されていることを確認
3. 「分類実行」ボタンをクリック
4. 処理状況を確認しながら待機
5. 結果表示で分類結果を確認

**期待される結果:**
- ID 1, 2: 分類結果 = 1（手関節の痛みに言及）
- ID 3: 分類結果 = 0（首の痛みのみで手関節には言及なし）

### テスト3: カスタムデータのアップロード

1. 独自のCSV/Excelファイルを準備
2. 「データ入力」でファイルをアップロード
3. カスタムプロンプトを入力
4. 分類実行して結果を確認
5. CSVダウンロードで結果を保存

## 設定のカスタマイズ

### config/config.yaml の編集

```yaml
model:
  name: "llama3:8b"          # 使用モデル
  temperature: 0.3            # 生成のランダム性
  max_tokens: 512             # 最大トークン数
  top_p: 0.9                  # Top-p サンプリング

ollama:
  host: "http://localhost:11434"  # Ollamaホスト
  timeout: 300                     # タイムアウト(秒)

gradio:
  server_name: "0.0.0.0"      # バインドアドレス
  server_port: 7860            # ポート番号
  share: false                 # Gradio公開共有
```

### モデルの変更

使用可能なOllamaモデルの例:
- `llama3:8b` (推奨、バランス型)
- `llama3:70b` (高精度、要大容量メモリ)
- `gemma:7b` (軽量)
- `mistral:7b` (軽量、高速)

設定パネルでリアルタイムにモデルを変更できます。

## トラブルシューティング

### Ollamaに接続できない

**症状:** 「接続失敗: Ollamaサーバーに接続できません」

**解決策:**
1. コンテナが完全に起動するまで30秒程度待つ
2. コンテナログを確認: `docker compose logs`
3. Ollamaサービスの状態確認: コンテナ内で `ps aux | grep ollama`

### メモリ不足エラー

**症状:** コンテナがクラッシュ、または非常に遅い

**解決策:**
1. Docker Desktopでメモリ割り当てを増やす（推奨: 8GB以上）
2. より軽量なモデルに変更（例: `gemma:7b`）

### ファイルアップロードエラー

**症状:** 「ファイル読み込みエラー」

**解決策:**
1. ファイル形式を確認（.csv, .xlsx, .xls のみ対応）
2. ファイル内に「text」または「テキスト」列が存在するか確認
3. 文字エンコーディングをUTF-8に変更

## 開発・拡張

### ローカル開発

Dockerを使わずにローカルで開発する場合:

```bash
# 依存関係のインストール
pip install -r requirements.txt

# Ollamaを別途インストール
curl -fsSL https://ollama.com/install.sh | sh

# Ollamaサーバー起動
ollama serve

# アプリケーション起動
python app/main.py
```

### コードの拡張

- `app/core/ollama_client.py`: LLM通信ロジック
- `app/core/classifier.py`: 分類アルゴリズム
- `app/ui/gradio_app.py`: UI定義
- `config/config.yaml`: 設定値

## ライセンス

このプロジェクトはMITライセンスの下で公開されています。

## 謝辞

- [Ollama](https://ollama.com/) - ローカルLLM実行環境
- [Gradio](https://gradio.app/) - 機械学習WebUI
- [Hydra](https://hydra.cc/) - 設定管理フレームワーク

## 今後の開発予定

- [ ] 多段階分類（複数選択肢からの分類）
- [ ] バッチ処理の最適化
- [ ] 進捗表示の強化
- [ ] API提供
- [ ] 分類精度の統計レポート
- [ ] マルチモデル比較機能

## お問い合わせ

バグ報告や機能要望は、GitHubのIssuesでお願いします。
